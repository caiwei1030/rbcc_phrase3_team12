"""
æœç´¢æœåŠ¡æ¨¡å— - å¤„ç†äº§å“åˆ†è§£å’Œé›¶ä»¶æœç´¢åŠŸèƒ½
"""

import streamlit as st
import requests
import json
import os
from config import LLM_MODEL, FASTGPT_API_KEY, FASTGPT_DATASET_ID
from .llm_service import get_llm_client, _generate_fallback_components, _calculate_relevance_reason


def _get_cad_image_path(part_id, source_file):
    """
    æ ¹æ®é›¶ä»¶IDå’Œæºæ–‡ä»¶åæŸ¥æ‰¾å¯¹åº”çš„CADå›¾ç‰‡è·¯å¾„
    """
    # CADå›¾ç‰‡ç›®å½•è·¯å¾„ - æ›´æ–°ä¸ºæ­£ç¡®çš„è·¯å¾„
    cad_images_dir = "cad2png/cad/cad/cad/images"
    
    # å°è¯•å¤šç§å¯èƒ½çš„å›¾ç‰‡æ–‡ä»¶å
    possible_names = [
        f"{part_id}.png",
        f"{source_file.replace('.json', '.png')}",
        f"{part_id.lower()}.png",
        f"{source_file.replace('.json', '').lower()}.png"
    ]
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    for img_name in possible_names:
        img_path = os.path.join(cad_images_dir, img_name)
        if os.path.exists(img_path):
            return img_path
    
    return None


def _load_cad_image_as_base64(image_path):
    """
    å°†CADå›¾ç‰‡åŠ è½½ä¸ºbase64ç¼–ç ï¼Œç”¨äºåœ¨Streamlitä¸­æ˜¾ç¤º
    """
    try:
        import base64
        with open(image_path, "rb") as img_file:
            img_data = img_file.read()
            img_base64 = base64.b64encode(img_data).decode('utf-8')
            return img_base64
    except Exception as e:
        st.warning(f"æ— æ³•åŠ è½½CADå›¾ç‰‡ {image_path}: {e}")
        return None


def _enhance_part_with_cad_image(part_data):
    """
    ä¸ºé›¶ä»¶æ•°æ®æ·»åŠ CADå›¾ç‰‡ä¿¡æ¯
    """
    part_id = part_data.get('part_number', part_data.get('id', ''))
    source_file = part_data.get('source_file', '')
    
    # æŸ¥æ‰¾å¯¹åº”çš„CADå›¾ç‰‡
    cad_image_path = _get_cad_image_path(part_id, source_file)
    
    if cad_image_path:
        # åŠ è½½å›¾ç‰‡ä¸ºbase64
        cad_image_base64 = _load_cad_image_as_base64(cad_image_path)
        if cad_image_base64:
            part_data['cad_image'] = cad_image_base64
            part_data['cad_image_path'] = cad_image_path
            part_data['has_cad_image'] = True
        else:
            part_data['has_cad_image'] = False
    else:
        part_data['has_cad_image'] = False
    
    return part_data


def find_parts_for_product(product_description: str):
    """
    å¢å¼ºçš„äº§å“åˆ†è§£åŠŸèƒ½ï¼š
    ç¬¬ä¸€æ­¥ï¼šLLMæ™ºèƒ½åˆ†è§£äº§å“ä¸ºé›¶ä»¶æ¸…å•ã€‚
    ç¬¬äºŒæ­¥ï¼šå¯¹æ¯ä¸ªé›¶ä»¶è¿›è¡Œå¤šè½®æœç´¢å’Œä¼˜åŒ–ã€‚
    ç¬¬ä¸‰æ­¥ï¼šåŸºäºæœç´¢ç»“æœè¿›ä¸€æ­¥ä¼˜åŒ–ç»„ä»¶åˆ—è¡¨ã€‚
    """
    llm_client = get_llm_client()
    if not llm_client:
        st.error("LLMæœåŠ¡ä¸å¯ç”¨ï¼Œæ— æ³•åˆ†è§£äº§å“ã€‚")
        return {}, [{"content": "LLMæœåŠ¡ä¸å¯ç”¨"}]

    # --- Step 1: Enhanced Product Decomposition ---
    st.info(f"ğŸ” æ­£åœ¨æ™ºèƒ½åˆ†æäº§å“: {product_description}...")
    
    # æ›´è¯¦ç»†å’Œæ™ºèƒ½çš„åˆ†è§£æç¤º
    decomposition_prompt = (
        "ä½ æ˜¯ä¸€ä¸ªèµ„æ·±çš„åˆ¶é€ å·¥ç¨‹å¸ˆã€äº§å“è®¾è®¡å¸ˆå’Œä¾›åº”é“¾ä¸“å®¶ã€‚\n"
        f"è¯·å°†ä»¥ä¸‹æˆå“è¿›è¡Œä¸“ä¸šçš„é›¶ä»¶åˆ†è§£: '{product_description}'\n\n"
        "åˆ†è§£åŸåˆ™:\n"
        "1. æŒ‰ç…§åˆ¶é€ å’Œè£…é…çš„é€»è¾‘å±‚æ¬¡åˆ†è§£\n"
        "2. è¯†åˆ«æ ¸å¿ƒåŠŸèƒ½ç»„ä»¶ã€ç»“æ„ç»„ä»¶å’Œè¿æ¥ç»„ä»¶\n"
        "3. è€ƒè™‘ææ–™ç±»å‹ã€åŠ å·¥å·¥è‰ºå’Œè£…é…æ–¹å¼\n"
        "4. ä¼˜å…ˆåˆ—å‡ºå¯é‡‡è´­çš„æ ‡å‡†ä»¶å’Œå®šåˆ¶ä»¶\n"
        "5. å¿½ç•¥è¿‡äºé€šç”¨çš„æ ‡å‡†ä»¶ï¼ˆå¦‚æ™®é€šèºä¸ï¼‰ï¼Œé™¤éæœ‰ç‰¹æ®Šè§„æ ¼è¦æ±‚\n"
        "6. ä½¿ç”¨å‡†ç¡®çš„å·¥ä¸šæœ¯è¯­å’Œè§„èŒƒåç§°\n"
        "7. è€ƒè™‘äº§å“çš„ä¸»è¦åŠŸèƒ½å’Œä½¿ç”¨åœºæ™¯\n\n"
        "è¾“å‡ºè¦æ±‚:\n"
        "- ä¸¥æ ¼ä½¿ç”¨JSONæ ¼å¼: {\"components\": [\"ç»„ä»¶1\", \"ç»„ä»¶2\", ...]}\n"
        "- ç»„ä»¶åç§°è¦å…·ä½“æ˜ç¡®ï¼Œä¾¿äºæœç´¢\n"
        "- æŒ‰é‡è¦æ€§æ’åºï¼Œæ ¸å¿ƒç»„ä»¶åœ¨å‰\n"
        "- æ§åˆ¶åœ¨5-15ä¸ªå…³é”®ç»„ä»¶\n"
        "- ä½¿ç”¨ç®€ä½“ä¸­æ–‡"
    )

    response_text = ""
    component_list = []
    
    try:
        resp = llm_client.chat.completions.create(
            model=LLM_MODEL,
            messages=[
                {"role": "system", "content": decomposition_prompt}
            ],
            max_tokens=1500,  # å¢åŠ tokené™åˆ¶
            temperature=0.1,
            response_format={"type": "json_object"}
        )
        response_text = resp.choices[0].message.content.strip()
        response_json = json.loads(response_text)
        
        # æ”¹è¿›çš„JSONè§£æé€»è¾‘
        if isinstance(response_json, dict):
            # æŸ¥æ‰¾åŒ…å«ç»„ä»¶åˆ—è¡¨çš„é”®
            possible_keys = ['components', 'parts', 'items', 'list', 'ç»„ä»¶', 'é›¶ä»¶']
            for key in possible_keys:
                if key in response_json and isinstance(response_json[key], list):
                    component_list = response_json[key]
                    break
            
            # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•å–ç¬¬ä¸€ä¸ªåˆ—è¡¨å€¼
            if not component_list:
                for value in response_json.values():
                    if isinstance(value, list) and value:
                        component_list = value
                        break
        elif isinstance(response_json, list):
            component_list = response_json
        
        # éªŒè¯å’Œæ¸…ç†ç»„ä»¶åˆ—è¡¨
        if component_list:
            component_list = [str(item).strip() for item in component_list if str(item).strip()]
            component_list = list(dict.fromkeys(component_list))  # å»é‡ä¿æŒé¡ºåº
        
        if not component_list:
            raise ValueError("æœªèƒ½ä»AIè¿”å›çš„JSONä¸­æå–å‡ºæœ‰æ•ˆçš„ç»„ä»¶åˆ—è¡¨")

    except (json.JSONDecodeError, ValueError) as e:
        st.error(f"ğŸš« AIåˆ†è§£äº§å“å¤±è´¥: {e}")
        st.error(f"è¿”å›å†…å®¹: {response_text}")
        
        # Fallback: å°è¯•åŸºäºäº§å“æè¿°ç”ŸæˆåŸºç¡€ç»„ä»¶åˆ—è¡¨
        fallback_components = _generate_fallback_components(product_description)
        if fallback_components:
            st.warning(f"ğŸ”„ ä½¿ç”¨å¤‡ç”¨åˆ†è§£æ–¹æ¡ˆï¼Œè¯†åˆ«å‡º {len(fallback_components)} ä¸ªåŸºç¡€ç»„ä»¶")
            component_list = fallback_components
        else:
            return {}, [{"content": "AIåˆ†è§£äº§å“å¤±è´¥", "error": f"{e} | è¿”å›å†…å®¹: {response_text}"}]
            
    except Exception as e:
        st.error(f"ğŸš« è°ƒç”¨LLMæ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        return {}, [{"content": "è°ƒç”¨LLMå¤±è´¥", "error": str(e)}]

    st.success(f"âœ… äº§å“åˆ†è§£å®Œæˆï¼Œè¯†åˆ«å‡º {len(component_list)} ä¸ªæ ¸å¿ƒç»„ä»¶: {', '.join(component_list)}")

    # --- Step 2: Enhanced Multi-round Search for Components ---
    final_results = {}
    errors = []
    
    progress_bar = st.progress(0)
    status_text = st.empty()

    for i, component in enumerate(component_list):
        status_text.text(f"ğŸ” æ­£åœ¨ä¸ºç»„ä»¶ '{component}' æœç´¢åŒ¹é…çš„é›¶ä»¶... ({i+1}/{len(component_list)})")
        
        try:
            # å¢å¼ºæœç´¢ç­–ç•¥ï¼Œä½¿ç”¨å¤šä¸ªæœç´¢å°è¯•æé«˜å¬å›ç‡
            all_parts = []
            search_attempts = [
                (component, 0.2),  # é™ä½ç›¸ä¼¼åº¦é˜ˆå€¼
                (component, 0.3),  # ä¸­ç­‰ç›¸ä¼¼åº¦é˜ˆå€¼
                (f"{component} é›¶ä»¶", 0.2),  # æ·»åŠ "é›¶ä»¶"å…³é”®è¯
                (f"{component} é…ä»¶", 0.2),  # æ·»åŠ "é…ä»¶"å…³é”®è¯
            ]
            
            seen_part_numbers = set()  # å»é‡ç”¨
            
            for search_query, similarity in search_attempts:
                try:
                    parts, search_errors = search_fastgpt_kb(search_query, similarity=similarity)
                    if search_errors:
                        errors.extend(search_errors)
                    if parts:
                        # å»é‡åˆå¹¶ç»“æœï¼ŒåŸºäºpart_number
                        for part in parts:
                            part_number = part.get('part_number', '')
                            if part_number not in seen_part_numbers:
                                seen_part_numbers.add(part_number)
                                all_parts.append(part)
                except Exception as e:
                    continue
            
            # æŒ‰åˆ†æ•°æ’åºï¼Œä¿ç•™æ›´ç›¸å…³çš„ç»“æœ
            if all_parts:
                all_parts.sort(key=lambda x: x.get('score', 0), reverse=True)
                final_results[component] = all_parts
                
        except Exception as e:
            errors.append({"content": f"æœç´¢ç»„ä»¶ '{component}' æ—¶å‡ºé”™", "error": str(e)})
            
        progress_bar.progress((i + 1) / len(component_list))

    progress_bar.empty()
    status_text.empty()
    
    # --- Step 3: Results Summary and Optimization ---
    total_parts_found = sum(len(parts) for parts in final_results.values())
    
    if final_results:
        st.info(f"ğŸ¯ æœç´¢å®Œæˆï¼ä¸º {len(final_results)} ä¸ªç»„ä»¶æ‰¾åˆ°äº† {total_parts_found} ä¸ªç›¸å…³é›¶ä»¶")
        
        # æ˜¾ç¤ºæœç´¢ç»Ÿè®¡
        with st.expander("ğŸ“Š æœç´¢ç»“æœç»Ÿè®¡", expanded=False):
            for component, parts in final_results.items():
                st.write(f"â€¢ **{component}**: {len(parts)} ä¸ªé›¶ä»¶")
    
    return final_results, errors


def search_fastgpt_kb(query: str, similarity: float = 0.5):
    """
    è°ƒç”¨FastGPTçŸ¥è¯†åº“è¿›è¡Œæœç´¢ï¼Œå¹¶æŒ‰å‘é‡ç›¸ä¼¼åº¦æ’åºã€‚
    å¢å¼ºçš„RAGæœç´¢åŠŸèƒ½ï¼Œæ”¯æŒå¤šè½®æŸ¥è¯¢å’Œç»“æœä¼˜åŒ–ã€‚
    """
    # æ£€æŸ¥é…ç½®æ˜¯å¦å®Œæ•´
    if not FASTGPT_API_KEY:
        error_msg = "FastGPT APIå¯†é’¥æœªé…ç½®ã€‚è¯·åœ¨Streamlit Cloudçš„ç¯å¢ƒå˜é‡ä¸­è®¾ç½®FASTGPT_API_KEYï¼Œæˆ–åœ¨æœ¬åœ°åˆ›å»º.streamlit/secrets.tomlæ–‡ä»¶ã€‚"
        st.error(error_msg)
        return [], [{"content": "é…ç½®é”™è¯¯", "error": error_msg}]
    
    if not FASTGPT_DATASET_ID:
        error_msg = "FastGPTæ•°æ®é›†IDæœªé…ç½®ã€‚è¯·åœ¨Streamlit Cloudçš„ç¯å¢ƒå˜é‡ä¸­è®¾ç½®FASTGPT_DATASET_IDï¼Œæˆ–åœ¨æœ¬åœ°åˆ›å»º.streamlit/secrets.tomlæ–‡ä»¶ã€‚"
        st.error(error_msg)
        return [], [{"content": "é…ç½®é”™è¯¯", "error": error_msg}]

    # ç›´æ¥ä½¿ç”¨åŸå§‹æŸ¥è¯¢ï¼Œä¸åšä»»ä½•é¢„å¤„ç†
    search_url = "https://cloud.fastgpt.cn/api/core/dataset/searchTest"
    headers = {
        "Authorization": f"Bearer {FASTGPT_API_KEY}",
        "Content-Type": "application/json"
    }
    
    # ä½¿ç”¨å¢å¼ºçš„æœç´¢å‚æ•°ï¼Œæé«˜å¬å›ç‡
    data = {
        "datasetId": FASTGPT_DATASET_ID,
        "text": query,
        "limit": 100,  # å¤§å¹…å¢åŠ æœç´¢ç»“æœæ•°é‡é™åˆ¶
        "similarity": max(0.1, similarity - 0.2),  # é™ä½ç›¸ä¼¼åº¦é˜ˆå€¼æé«˜å¬å›ç‡
        "searchMode": "mixedRecall",
        "usingReRank": True,
        "datasetSearchUsingExtensionQuery": True,  # å¼€å¯æŸ¥è¯¢æ‰©å±•ä»¥æé«˜åŒ¹é…
        "datasetSearchExtensionModel": "",
        "datasetSearchExtensionBg": ""
    }

    try:
        response = requests.post(search_url, headers=headers, json=data, timeout=30)
        response.raise_for_status()

        try:
            response_data = response.json()
        except json.JSONDecodeError:
            st.error(f"FastGPT APIå“åº”ä¸æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼: {response.text}")
            return [], [{"content": "APIå“åº”æ ¼å¼é”™è¯¯", "error": response.text}]

        if not isinstance(response_data, dict):
            error_msg = str(response_data)
            st.error(f"FastGPT APIè¿”å›äº†éé¢„æœŸçš„æ ¼å¼: {error_msg}")
            return [], [{"content": "APIè¿”å›æ ¼å¼é”™è¯¯", "error": error_msg}]

        if response_data.get("code") == 200:
            data_payload = response_data.get("data", {})
            search_results = data_payload.get("list", [])

            if not search_results:
                return [], []

            processed_results = []
            for result in search_results:
                try:
                    # æ”¹è¿›çš„åˆ†æ•°å¤„ç†é€»è¾‘
                    score_list = result.get('score', [])
                    final_score = 0.0
                    rerank_score = 0.0
                    
                    if isinstance(score_list, list):
                        for score_item in score_list:
                            score_type = score_item.get('type', '')
                            score_value = score_item.get('value', 0.0)
                            
                            if score_type == 'embedding':
                                final_score = score_value
                            elif score_type == 'rerank':
                                rerank_score = score_value
                    elif isinstance(score_list, (int, float)):
                        final_score = score_list
                    
                    # ç»¼åˆè¯„åˆ†ï¼šä¼˜å…ˆä½¿ç”¨rerankåˆ†æ•°ï¼Œfallbackåˆ°embeddingåˆ†æ•°
                    combined_score = rerank_score if rerank_score > 0 else final_score

                    q_content = result.get('q', '').strip()
                    if not q_content:
                        continue
                    
                    json_start_index = q_content.find('{')
                    if json_start_index == -1:
                        continue
                    json_string = q_content[json_start_index:]
                    part_data = json.loads(json_string)

                    # å¢å¼ºæ•°æ®æå–å’Œæ¸…ç†
                    normalized_part = {
                        'part_number': str(part_data.get('part_number', part_data.get('id', 'N/A'))),
                        'part_name': str(part_data.get('part_name', part_data.get('source_file', 'N/A'))),
                        'description': str(part_data.get('description', 'æ— æè¿°')),
                        'operator': str(part_data.get('operator', 'ç³»ç»Ÿ')),
                        'created_time': str(part_data.get('created_time', 'æœªçŸ¥')),
                        'image': part_data.get('image'),
                        'source_file': str(part_data.get('source_file', '')),
                        'keywords': str(part_data.get('keywords', '')),
                        'score': combined_score,
                        'embedding_score': final_score,
                        'rerank_score': rerank_score,
                        'original_score': final_score,
                        'llm_analyzed': False,
                        'relevance_reason': _calculate_relevance_reason(query, part_data)
                    }
                    
                    # å¢å¼ºé›¶ä»¶æ•°æ®ï¼Œæ·»åŠ CADå›¾ç‰‡ä¿¡æ¯
                    normalized_part = _enhance_part_with_cad_image(normalized_part)
                    
                    # ä¿ç•™æ‰€æœ‰ç»“æœï¼Œè®©FastGPTçš„ç›¸ä¼¼åº¦é˜ˆå€¼èµ·ä½œç”¨
                    processed_results.append(normalized_part)
                        
                except (json.JSONDecodeError, TypeError) as e:
                    st.warning(f"è·³è¿‡ä¸€ä¸ªæ— æ³•è§£æçš„ç»“æœ: {e}")
                    continue
            
            # ç®€å•æ’åºï¼šæŒ‰ç»¼åˆåˆ†æ•°é™åº
            processed_results.sort(key=lambda x: x.get('score', 0), reverse=True)
            
            return processed_results, []
        else:
            error_msg = response_data.get('message', 'æœªçŸ¥APIé”™è¯¯')
            st.error(f"FastGPT API é”™è¯¯: {error_msg}")
            return [], [{"content": "APIè¿”å›é”™è¯¯", "error": error_msg}]

    except requests.exceptions.RequestException as e:
        st.error(f"è¯·æ±‚FastGPTå¤±è´¥: {e}")
        return [], [{"content": "è¯·æ±‚å¤±è´¥", "error": str(e)}]
    except Exception as e:
        st.error(f"å¤„ç†FastGPTç»“æœæ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        return [], [{"content": "å¤„ç†è¿‡ç¨‹ä¸­å‘ç”ŸæœªçŸ¥é”™è¯¯", "error": str(e)}]